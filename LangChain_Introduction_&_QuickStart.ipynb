{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8a8c44",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "LangChain is a framework for developing applications powered by language models. It enables applications that are:\n",
    "\n",
    "    Data-aware: connect a language model to other sources of data\n",
    "    Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The main value props of LangChain are:\n",
    "\n",
    "    1. Components: abstractions for working with language models, along with a collection of implementations for each   abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n",
    "\n",
    "    2. Off-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks\n",
    "    Off-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it   easy   to customize existing chains or build new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880b1c5",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdd0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install this library\n",
    "#!pip install langchain\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68802f",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b98ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = 'Enter-API-KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72172023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"Enter-API-KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd3a8d",
   "metadata": {},
   "source": [
    "# Building an application\n",
    "Now we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fb4c9",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "### Get predictions from a language model\n",
    "The basic building block of LangChain is the LLM, which takes in text and generates more text.\n",
    "\n",
    "As an example, suppose we're building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we'll initialize our model with a HIGH temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c264501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"Enter-API-KEY\",temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6ccdb",
   "metadata": {},
   "source": [
    "And now we can pass in text and get predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf592de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nColorful Cozies.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"What would be a good company name for a company that makes colorful socks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba789ed",
   "metadata": {},
   "source": [
    "# Chat models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316c0f7",
   "metadata": {},
   "source": [
    "Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n",
    "\n",
    "You can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f150c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c37a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(lc_kwargs={'content': \"J'aime programmer.\"}, content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(openai_api_key=\"Enter-API-KEY\",temperature=0)\n",
    "chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b730f81",
   "metadata": {},
   "source": [
    "It is useful to understand how chat models are different from a normal LLM, but it can often be handy to just be able to treat them the same. LangChain makes that easy by also exposing an interface through which you can interact with a chat model as you would a normal LLM. You can access this through the predict interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc2cf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime programmer.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"Translate this sentence from English to French. I love programming.\")\n",
    "# >> J'aime programmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9687f6d",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "Most LLM applications do not pass user input directly into to an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n",
    "\n",
    "In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "887d1d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82404325",
   "metadata": {},
   "source": [
    "# Chains\n",
    "Now that we've got a model and a prompt template, we'll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952040f6",
   "metadata": {},
   "source": [
    "The simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc4ab6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nRainbow Toes.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc2466c",
   "metadata": {},
   "source": [
    "# Agents\n",
    "Our first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs.\n",
    "\n",
    "Agents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer.\n",
    "\n",
    "To load an agent, you need to choose a(n):\n",
    "\n",
    "   1. LLM/Chat model: The language model powering the agent.\n",
    "   2. Tool(s): A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the Tools documentation.\n",
    "   3. Agent name: A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de796ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\irfan\\anaconda3\\lib\\site-packages (from google-search-results) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\irfan\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\irfan\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\irfan\\anaconda3\\lib\\site-packages (from requests->google-search-results) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\irfan\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2022.12.7)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py): started\n",
      "  Building wheel for google-search-results (setup.py): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32077 sha256=e931d37becee508066c37bce071626aba289cf9489246fb225e7f75162a04a94\n",
      "  Stored in directory: c:\\users\\irfan\\appdata\\local\\pip\\cache\\wheels\\de\\1b\\db\\6474e3f9e34a03ca54b85d98fc7742001e6fae1ff3881e3ed4\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "# You'll need to install the SerpAPI Python package:\n",
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb92858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SERPAPI_API_KEY=525fe40634dc8e328ce2981597c9cd6b579ae2f54fa84bfcc4c48f368b31acdf\n"
     ]
    }
   ],
   "source": [
    "%env SERPAPI_API_KEY=525fe40634dc8e328ce2981597c9cd6b579ae2f54fa84bfcc4c48f368b31acdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6b6e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: Search\n",
      "Action Input: \"High temperature in SF yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHigh: 66.2ºf @1:40 PM Low: 55.04ºf @5:56 AM Approx.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to use the calculator to raise 66.2 to the .023 power\n",
      "Action: Calculator\n",
      "Action Input: 66.2^.023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1012343099196273\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1.1012343099196273\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1012343099196273'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# The language model we're going to use to control the agent.\n",
    "llm = OpenAI(openai_api_key=\"Enter-API-KEY\",temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62362d79",
   "metadata": {},
   "source": [
    "# Memory\n",
    "The chains and agents we've looked at so far have been stateless, but for many applications it's necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages.\n",
    "\n",
    "The Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state.\n",
    "\n",
    "There are a number of built-in memory systems. The simplest of these are is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aabc3d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"Enter-API-KEY\",temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.run(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b98b8dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7efefad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
      "Human: Do You know, Who am I?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know who you are. Could you tell me a bit more about yourself?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Do You know, Who am I?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56ea94e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
      "Human: Do You know, Who am I?\n",
      "AI:  I'm sorry, I don't know who you are. Could you tell me a bit more about yourself?\n",
      "Human: I am a lost guy, I am finding my self.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I see. It can be difficult to find yourself, but it can also be a rewarding journey. What have you been doing to help you on your journey?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"I am a lost guy, I am finding my self.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16af67ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
      "Human: Do You know, Who am I?\n",
      "AI:  I'm sorry, I don't know who you are. Could you tell me a bit more about yourself?\n",
      "Human: I am a lost guy, I am finding my self.\n",
      "AI:  I see. It can be difficult to find yourself, but it can also be a rewarding journey. What have you been doing to help you on your journey?\n",
      "Human: Suggest me from where I need to start my journey?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  That's a great question! It really depends on what you're looking for. What kind of things do you want to explore on your journey?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Suggest me from where I need to start my journey?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b002f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
